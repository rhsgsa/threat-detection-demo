apiVersion: serving.kserve.io/v1alpha1
kind: ServingRuntime
metadata:
  annotations:
    opendatahub.io/apiProtocol: REST
    opendatahub.io/accelerator-name: managed-gpu
    opendatahub.io/recommended-accelerators: '["nvidia.com/gpu"]'
    opendatahub.io/template-display-name: vLLM OpenAI ServingRuntime for KServe
    opendatahub.io/template-name: vllm-openai-runtime
    openshift.io/display-name: mistral
  labels:
    opendatahub.io/dashboard: "true"
  name: mistral
spec:
  annotations:
    prometheus.io/port: "8080"
    prometheus.io/path: "/metrics"
  multiModel: false
  supportedModelFormats:
  - name: vllm
    autoSelect: true
  containers:
  - name: kserve-container
    image: docker.io/vllm/vllm-openai:v0.4.0
    #image: docker.io/kserve/vllmserver:latest
    workingDir: /root
    command:
    - bash
    - "-c"
    - |
      GPU_COUNT="$(echo -n $NVIDIA_VISIBLE_DEVICES | awk -F, '{ print NF }')"
      if [ -z "$GPU_COUNT" ]; then
        GPU_COUNT=1
      fi
      echo "number of gpus=$GPU_COUNT"
      echo "extra args=$EXTRA_ARGS"
      exec python3 -m vllm.entrypoints.openai.api_server \
        --port 8080 \
        --model /mnt/models \
        --tensor-parallel-size $GPU_COUNT $EXTRA_ARGS
    ports:
    - containerPort: 8080
      protocol: TCP
    env:
    - name: HOME # needed because we need to write to $HOME/.cache
      value: /root
    - name: PYTHONPATH
      value: /workspace
    volumeMounts:
    - name: home
      mountPath: /root
    - name: shm
      mountPath: /dev/shm
    livenessProbe:
      httpGet:
        path: /version
        port: 8080
      initialDelaySeconds: 240
    readinessProbe:
      httpGet:
        path: /version
        port: 8080
      initialDelaySeconds: 150
    startupProbe:
      httpGet:
        path: /version
        port: 8080
      failureThreshold: 60
      periodSeconds: 10
  volumes:
  - name: home
    emptyDir: {}
  - name: shm
    emptyDir:
      medium: Memory
      sizeLimit: 2Gi
  tolerations:
    - key: nvidia.com/gpu
      operator: "Exists"
      effect: NoSchedule
